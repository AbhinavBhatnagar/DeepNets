{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Motivated from the work of @dandxy89   https://github.com/dandxy89/DeepLearning_MachineLearning/tree/master/EmbeddingKeras    \n",
    "\n",
    " <br />\n",
    "\n",
    "* **Train your own w2v model** on the dataset vocab  \n",
    "* Use the **w2v vectors to initialise the weights of the embedding layer**  \n",
    "* This is **2-stacked lstm**  (This is the only diff between this notebook and IMDB_Embedding_w2v_LSTM_1)\n",
    "* This is **stateless lstm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (10,10) # Make the figures a bit bigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GRID K520 (CNMeM is disabled, CuDNN 4004)\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "import numpy as np\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.corpora.dictionary import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "/home/ubuntu/.virtualenvs/keras_0_3_2/lib/python2.7/site-packages/theano/tensor/signal/downsample.py:5: UserWarning: downsample module has been moved to the pool module.\n",
      "  warnings.warn(\"downsample module has been moved to the pool module.\")\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.core import Dense, Dropout\n",
    "np.random.seed(1337)  # For Reproducibility\n",
    "\n",
    "from itertools import chain\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "data_locations = {'./Data/TRAIN_NEG.txt': 'TRAIN_NEG',\n",
    "                  './Data/TRAIN_POS.txt': 'TRAIN_POS',\n",
    "                  './Data/TEST_NEG.txt': 'TEST_NEG',\n",
    "                  './Data/TEST_POS.txt': 'TEST_POS'}\n",
    "'''\n",
    "\n",
    "data_locations = {'./Data/test-neg.txt': 'TEST_NEG',\n",
    "                  './Data/test-pos.txt': 'TEST_POS',\n",
    "                  './Data/train-neg.txt': 'TRAIN_NEG',\n",
    "                  './Data/train-pos.txt': 'TRAIN_POS'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def import_tag(datasets=None):\n",
    "    ''' Imports the datasets into one of two dictionaries\n",
    "\n",
    "        Dicts:\n",
    "            train & test\n",
    "\n",
    "        Keys:\n",
    "            values >= 4  are \"Positive\" in both Dictionaries\n",
    "\n",
    "        '''\n",
    "    if datasets is not None:\n",
    "        train = {}\n",
    "        test = {}\n",
    "        for k, v in datasets.items(): #gives a list of tuples to iterate\n",
    "            with open(k) as fpath:\n",
    "                data = fpath.readlines()\n",
    "            for val, each_line in enumerate(data):\n",
    "                if v.endswith(\"NEG\") and v.startswith(\"TRAIN\"):\n",
    "                    train[val] = each_line\n",
    "                elif v.endswith(\"POS\") and v.startswith(\"TRAIN\"):\n",
    "                    train[val + 12500] = each_line\n",
    "                elif v.endswith(\"NEG\") and v.startswith(\"TEST\"):\n",
    "                    test[val] = each_line\n",
    "                else:\n",
    "                    test[val + 12500] = each_line\n",
    "        return train, test\n",
    "    else:\n",
    "        print('Data not found...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, test = import_tag(data_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    ''' Simple Parser converting each document to lower-case, then\n",
    "        removing the breaks for new lines and finally splitting on the\n",
    "        whitespace\n",
    "    '''\n",
    "    text = [document.lower().replace('\\n', '').split() for document in text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data...\n",
      "Tokenising...\n"
     ]
    }
   ],
   "source": [
    "print('Loading Data...')\n",
    "combined = train.values() + test.values()\n",
    "print('Tokenising...')\n",
    "combined = tokenizer(combined)\n",
    "#combined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training a Word2vec model...\n",
      "Word2vec model trained...\n"
     ]
    }
   ],
   "source": [
    "# here we are training our own w2v model. we can as well use a pretrained model\n",
    "\n",
    "# Word2Vec model parameters\n",
    "vocab_dim = 300\n",
    "n_exposures = 30 \n",
    "window_size = 7 \n",
    "n_iterations = 1  # ideally more..\n",
    "cpu_count = multiprocessing.cpu_count() # number of workers\n",
    "\n",
    "print('Training a Word2vec model...')\n",
    "w2v_model = Word2Vec(size=vocab_dim,\n",
    "                 min_count=n_exposures,\n",
    "                 window=window_size,\n",
    "                 workers=cpu_count,\n",
    "                 iter=n_iterations)\n",
    "w2v_model.build_vocab(combined)\n",
    "w2v_model.train(combined)\n",
    "print('Word2vec model trained...')\n",
    "# --- by now a model should be ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#  get the vocab size of w2v_model\n",
    "len(w2v_model.vocab.keys())\n",
    "\n",
    "# this is one way to create dataset vocab\n",
    "flat_list = chain(*combined)\n",
    "dataset_vocab = set(flat_list)\n",
    "len(dataset_vocab)\n",
    "dataset_vocab_lower = [key.lower() for key in dataset_vocab]\n",
    "       \n",
    "#counter = 0\n",
    "#for key in dataset_vocab_lower:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100673\n",
      "85306\n"
     ]
    }
   ],
   "source": [
    "DIMENSION = 300\n",
    "zero_vector = np.zeros(DIMENSION)\n",
    "\n",
    "our_dict = {}\n",
    "missing_count = 0\n",
    "\n",
    "for key in dataset_vocab_lower:\n",
    "    if key in w2v_model:\n",
    "        our_dict[key] = w2v_model[key]\n",
    "    else:\n",
    "        missing_count += 1\n",
    "        #print(\"missing key %s\" %(key)) \n",
    "        our_dict[key] = zero_vector\n",
    "\n",
    "print len(dataset_vocab_lower)\n",
    "print missing_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# process the dataset - all that needs to be done is: replace every word in dataset by its id = (token_id + 1)\n",
    "# for token_id, refer to create_dictionaries()\n",
    "def parse_dataset(data, w2indx):\n",
    "    ''' \n",
    "        Transforms the Training and Testing Dictionaries - Words become integers. This converts each sentence into\n",
    "        a bunch of integers just like imdb dataset in keras.\n",
    "        Input: Data (dictionary) and w2indx (dictionary)\n",
    "    '''\n",
    "    for key in data.keys():\n",
    "        txt = data[key].lower().replace('\\n', '').split()\n",
    "        new_txt = []\n",
    "        for word in txt:\n",
    "            try:\n",
    "                new_txt.append(w2indx[word])\n",
    "            except:\n",
    "                new_txt.append(0)\n",
    "        data[key] = new_txt\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dictionaries(train=None,\n",
    "                        test=None,\n",
    "                        model=None):\n",
    "    ''' Function does are number of Jobs:\n",
    "        1- Creates a word to index mapping\n",
    "        2- Creates a word to vector mapping\n",
    "    '''\n",
    "    if (train is not None) and (model is not None) and (test is not None):\n",
    "        gensim_dict = Dictionary() # initialise empty gensim.corpora.dictionary\n",
    "        gensim_dict.doc2bow(w2v_model.vocab.keys(), \n",
    "                            allow_update=True)  # Converts words in model.vocab into the bag-of-words \n",
    "                                                # format = list of (token_id, token_frequency_count) 2-tuples\n",
    "                                                # token_id is a running integer counter \n",
    "        \n",
    "        w2indx = {v: k+1 for k, v in gensim_dict.items()} # create a dictionary with {word: (token_id + 1)}\n",
    "        # [+1 is keep 0 reserved for no word in w2c model]\n",
    "        \n",
    "        w2vec = {word: model[word] for word in w2indx.keys()} #create a dictionary with {word: word_vector}\n",
    "        \n",
    "        #train = parse_dataset(train, w2indx)\n",
    "        #test = parse_dataset(test, w2indx)\n",
    "        return w2indx, w2vec\n",
    "    else:\n",
    "        print('No data provided...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform the Data...\n"
     ]
    }
   ],
   "source": [
    "print('Transform the Data...')\n",
    "index_dict, word_vectors = create_dictionaries(train=train, test=test, model=our_dict)\n",
    "train = parse_dataset(train, index_dict)\n",
    "test = parse_dataset(test, index_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Arrays for Keras Embedding Layer...\n"
     ]
    }
   ],
   "source": [
    "print('Setting up Arrays for Keras Embedding Layer...')\n",
    "n_symbols = len(index_dict) + 1  # adding 1 to account for 0th index\n",
    "embedding_weights = np.zeros((n_symbols + 1, vocab_dim))\n",
    "\n",
    "for word, index in index_dict.items():\n",
    "    embedding_weights[index, :] = word_vectors[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Datesets...\n"
     ]
    }
   ],
   "source": [
    "print('Creating Datesets...')\n",
    "X_train = train.values()\n",
    "y_train = [1 if value > 12500 else 0 for value in train.keys()]\n",
    "X_test = test.values()\n",
    "y_test = [1 if value > 12500 else 0 for value in test.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data parameters\n",
    "maxlen = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "('X_train shape:', (25000, 100))\n",
      "('X_test shape:', (25000, 100))\n",
      "Convert labels to Numpy Sets...\n"
     ]
    }
   ],
   "source": [
    "print(\"Pad sequences (samples x time)\")\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "\n",
    "print('Convert labels to Numpy Sets...')\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Embedding layer parameter\n",
    "input_length = 100\n",
    "\n",
    "# training parameters\n",
    "batch_size = 32\n",
    "n_epoch = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining a Simple Keras Model...\n",
      "Compiling the Model...\n"
     ]
    }
   ],
   "source": [
    "print('Defining a Simple Keras Model...')\n",
    "dl_model = Sequential()  # or Graph or whatever\n",
    "dl_model.add(Embedding(output_dim=vocab_dim,\n",
    "                    input_dim=n_symbols + 1,\n",
    "                    mask_zero=True,\n",
    "                    weights=[embedding_weights],\n",
    "                    input_length=input_length))  # Adding Input Length\n",
    "\n",
    "# A LSTM unit takes as input a sequence, but by default it doesn't return a sequence. To stack them, \n",
    "#you have to configure intermediate LSTM units to return sequences, using the constructor argument \n",
    "#\"return_sequences=True\"\n",
    "\n",
    "dl_model.add(LSTM(512, return_sequences=True))  # returns a sequence of vectors of dimension 512\n",
    "dl_model.add(Dropout(0.3))\n",
    "dl_model.add(LSTM(512, return_sequences=True)) # returns a sequence of vectors of dimension 512\n",
    "dl_model.add(Dropout(0.3))\n",
    "dl_model.add(LSTM(512, return_sequences=False)) # return a single vector of dimension 512\n",
    "dl_model.add(Dropout(0.3))\n",
    "dl_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "print('Compiling the Model...')\n",
    "dl_model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              class_mode='binary')\n",
    "print('Model Compiled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Train...\")\n",
    "start_time = time()\n",
    "hist = dl_model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=n_epoch,\n",
    "          verbose=2, validation_data=(X_test, y_test), show_accuracy=True)\n",
    "end_time = time()\n",
    "\n",
    "print(\"Training took %f secs\" %(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Evaluate...\")\n",
    "loss, acc = dl_model.evaluate(X_test, y_test,\n",
    "                            batch_size=batch_size,\n",
    "                            show_accuracy=True)\n",
    "print('Test score:', loss)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "\n",
    "axes = plt.gca()\n",
    "x_min = hist.epoch[0]\n",
    "x_max = hist.epoch[-1]+1\n",
    "axes.set_xlim([x_min,x_max])\n",
    "\n",
    "plt.scatter(hist.epoch, hist.history['loss'], color='g')\n",
    "plt.plot(hist.history['loss'], color='g', label='Training Loss')\n",
    "plt.scatter(hist.epoch, hist.history['val_loss'], color='b')\n",
    "plt.plot(hist.history['val_loss'], color='b', label='Validation Loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss & Validation Loss vs Epochs')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure(2)\n",
    "\n",
    "axes = plt.gca()\n",
    "x_min = hist.epoch[0]\n",
    "x_max = hist.epoch[-1]+1\n",
    "axes.set_xlim([x_min,x_max])\n",
    "\n",
    "plt.scatter(hist.epoch, hist.history['acc'], color='r')\n",
    "plt.plot(hist.history['acc'], color='r', label='Training Accuracy')\n",
    "plt.scatter(hist.epoch, hist.history['val_acc'], color='c')\n",
    "plt.plot(hist.history['val_acc'], color='c', label='Validation Accuracy')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Trainging Accuracy & Validation Accuracy vs Epochs')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
